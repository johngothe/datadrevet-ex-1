{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6906958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc47508f",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_file = 'SD.csv'\n",
    "old = pd.read_csv(old_file)\n",
    "\n",
    "file = 'filtered_data.csv'\n",
    "df = pd.read_csv(file)\n",
    "\n",
    "\n",
    "print(len(old))\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5aebc1a",
   "metadata": {},
   "source": [
    "Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3082ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41d1b1d",
   "metadata": {},
   "source": [
    "Datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8058d553",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af729b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())\n",
    "# No null values but if there were a few we would drop them\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b05a79c",
   "metadata": {},
   "source": [
    "# Outliers #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6bc0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and display outliers using IQR method for numerical columns\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "for col in numerical_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "    \n",
    "    print(f\"{col}:\")\n",
    "    print(f\"  Outlier count: {len(outliers)}\")\n",
    "    print(f\"  Outlier percentage: {len(outliers)/len(df)*100:.2f}%\")\n",
    "    print(f\"  Range: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "    if len(outliers) > 0:\n",
    "        print(f\"  Min outlier: {outliers[col].min():.2f}\")\n",
    "        print(f\"  Max outlier: {outliers[col].max():.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabab73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations for outliers\n",
    "fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(numerical_cols[:12]):  # Show first 12 numerical columns\n",
    "    # Calculate outlier bounds for this column\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Create box plot\n",
    "    sns.boxplot(data=df, y=col, ax=axes[i])\n",
    "    axes[i].set_title(f'{col} - Outliers Detection')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create additional plots for remaining numerical columns if any\n",
    "if len(numerical_cols) > 12:\n",
    "    remaining_cols = numerical_cols[12:]\n",
    "    n_remaining = len(remaining_cols)\n",
    "    n_cols = 4\n",
    "    n_rows = (n_remaining + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5*n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = [axes] if n_remaining == 1 else axes\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for i, col in enumerate(remaining_cols):\n",
    "        sns.boxplot(data=df, y=col, ax=axes[i])\n",
    "        axes[i].set_title(f'{col} - Outliers Detection')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for j in range(i+1, len(axes)):\n",
    "        axes[j].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "148a64f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Variable groups from dataset documentation ---\n",
    "CONTINUOUS_VARS = [\n",
    "    \"age\", \"height\", \"weight\", \"sight_left\", \"sight_right\",\n",
    "    \"SBP\", \"DBP\", \"BLDS\", \"tot_chole\", \"HDL_chole\", \"LDL_chole\",\n",
    "    \"triglyceride\", \"hemoglobin\", \"serum_creatinine\",\n",
    "    \"SGOT_AST\", \"SGOT_ALT\", \"gamma_GTP\", \"waistline\"\n",
    "]\n",
    "\n",
    "ORDINAL_VARS = [\"urine_protein\"]  # exclude from IQR\n",
    "CATEGORICAL_VARS = [\"sex\", \"hear_left\", \"hear_right\", \"SMK_stat_type_cd\", \"DRK_YN\"]\n",
    "\n",
    "# Use intersection with actual columns\n",
    "cont_cols = [c for c in CONTINUOUS_VARS if c in df.columns]\n",
    "\n",
    "def iqr_bounds(s: pd.Series):\n",
    "    s = pd.to_numeric(s, errors=\"coerce\").dropna()\n",
    "    q1, q3 = s.quantile([0.25, 0.75])\n",
    "    iqr = q3 - q1\n",
    "    low = q1 - 1.5 * iqr\n",
    "    high = q3 + 1.5 * iqr\n",
    "    return low, high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249c30ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove (clear errors)\n",
    "\n",
    "df_clean = df.copy()\n",
    "\n",
    "REMOVE_RULES = {\n",
    "    \"waistline\": lambda s: s > 200,  # e.g., 1000 cm entries\n",
    "    \"serum_creatinine\": lambda s: s > 20,  # mg/dL, implausible highs\n",
    "    \"SGOT_AST\": lambda s: s > 9000,  # extreme enzyme typos\n",
    "    \"SGOT_ALT\": lambda s: s > 9000,\n",
    "    \"sight_left\": lambda s: s >= 5,  # 10.0 eyesight entries -> remove\n",
    "    \"sight_right\": lambda s: s >= 5,\n",
    "}\n",
    "\n",
    "# Apply removal rules\n",
    "mask_keep = pd.Series(True, index=df_clean.index)\n",
    "remove_report = []\n",
    "for col, rule in REMOVE_RULES.items():\n",
    "    if col in df_clean.columns:\n",
    "        bad = rule(pd.to_numeric(df_clean[col], errors=\"coerce\"))\n",
    "        n_bad = int(bad.sum())\n",
    "        if n_bad > 0:\n",
    "            mask_keep &= ~bad\n",
    "            remove_report.append((col, n_bad))\n",
    "\n",
    "df_clean = df_clean.loc[mask_keep]\n",
    "print(\"Rows removed (by column):\", remove_report)\n",
    "\n",
    "# =========================\n",
    "# 2) CAP (winsorize at IQR)\n",
    "# =========================\n",
    "cap_report = []\n",
    "for col in cont_cols:\n",
    "    if col in df_clean.columns:\n",
    "        low, high = iqr_bounds(df_clean[col])\n",
    "        before = pd.to_numeric(df_clean[col], errors=\"coerce\")\n",
    "        n_low = int((before < low).sum())\n",
    "        n_high = int((before > high).sum())\n",
    "        df_clean[col] = before.clip(lower=low, upper=high)\n",
    "        cap_report.append({\n",
    "            \"column\": col,\n",
    "            \"capped_low\": n_low,\n",
    "            \"capped_high\": n_high\n",
    "        })\n",
    "\n",
    "print(pd.DataFrame(cap_report))\n",
    "\n",
    "# ==============================\n",
    "# 3) TRANSFORM (reduce skew with log1p)\n",
    "# ==============================\n",
    "LOG1P_VARS = [\n",
    "    v for v in [\n",
    "        \"weight\", \"BLDS\", \"tot_chole\", \"HDL_chole\", \"LDL_chole\",\n",
    "        \"triglyceride\", \"SGOT_AST\", \"SGOT_ALT\", \"gamma_GTP\"\n",
    "    ]\n",
    "    if v in df_clean.columns\n",
    "]\n",
    "\n",
    "transform_report = []\n",
    "for col in LOG1P_VARS:\n",
    "    x = pd.to_numeric(df_clean[col], errors=\"coerce\")\n",
    "    n_neg = int((x < 0).sum())\n",
    "    if n_neg:\n",
    "        x = x.clip(lower=0)  # ensure non-negative before log1p\n",
    "    df_clean[col] = np.log1p(x)\n",
    "    transform_report.append({\n",
    "        \"column\": col,\n",
    "        \"negatives_clipped\": n_neg\n",
    "    })\n",
    "\n",
    "print(pd.DataFrame(transform_report))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fa0759",
   "metadata": {},
   "source": [
    "# Feature extracting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc25e9e",
   "metadata": {},
   "source": [
    "Feature extraction after stabilizing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae9775c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derived features (feature extraction)\n",
    "df_clean['BMI'] = df_clean['weight'] / (df_clean['height'] / 100) ** 2\n",
    "df_clean['pulse_pressure'] = df_clean['SBP'] - df_clean['DBP']\n",
    "df_clean['LDL_to_HDL'] = df_clean['LDL_chole'] / df_clean['HDL_chole']\n",
    "df_clean['TG_to_HDL'] = df_clean['triglyceride'] / df_clean['HDL_chole']\n",
    "df_clean['sight_avg'] = (df_clean['sight_left'] + df_clean['sight_right']) / 2\n",
    "df_clean['hearing_avg'] = (df_clean['hear_left'] + df_clean['hear_right']) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1c7eb3",
   "metadata": {},
   "source": [
    "Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7216e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy to avoid changing the original dataset\n",
    "df_encoded = df_clean.copy()\n",
    "df_encoded.drop(columns=['hear_left','hear_right','sight_left','sight_right'], inplace=True)\n",
    "\n",
    "# Ordinal encoding: urine_protein (1 to 6 is ordered)\n",
    "df_encoded[\"urine_protein\"] = df_encoded[\"urine_protein\"].astype(int)\n",
    "\n",
    "# One-hot encoding for nominal variables\n",
    "nominal_vars = [\"sex\", \"SMK_stat_type_cd\", \"DRK_YN\"]\n",
    "\n",
    "df_encoded = pd.get_dummies(df_encoded, columns=nominal_vars, drop_first=True)\n",
    "\n",
    "# Show only the newly created encoded columns\n",
    "encoded_cols = [col for col in df_encoded.columns if any(var in col for var in nominal_vars)] + [\"urine_protein\"]\n",
    "print(df_encoded[encoded_cols].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36808b0",
   "metadata": {},
   "source": [
    "Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd2e63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set 'DRK_YN' (Drinker or Not) as target variable — use the encoded dataframe\n",
    "# If one-hot encoding created \"DRK_YN_Y\", use that as your target\n",
    "target_col = \"DRK_YN_Y\" if \"DRK_YN_Y\" in df_encoded.columns else \"DRK_YN\"\n",
    "\n",
    "# Separate features (X) and label (y)\n",
    "X = df_encoded.drop(columns=[target_col])\n",
    "y = df_encoded[target_col]\n",
    "\n",
    "# Split the data into training and testing sets (80–20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Training set shape (X_train):\", X_train.shape)\n",
    "print(\"Testing set shape (X_test):\", X_test.shape)\n",
    "print(\"Training set shape (y_train):\", y_train.shape)\n",
    "print(\"Testing set shape (y_test):\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd5df75",
   "metadata": {},
   "source": [
    "### Correlation filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5c1839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1 Correlation-based filtering ---\n",
    "corr_df = X_train.copy()\n",
    "corr_df['target'] = y_train\n",
    "\n",
    "corr_matrix = corr_df.corr(numeric_only=True)\n",
    "corr_with_target = corr_matrix['target'].drop('target').sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8,10))\n",
    "sns.barplot(x=corr_with_target.values, y=corr_with_target.index, palette=\"viridis\")\n",
    "plt.title(\"Feature correlations with alcohol consumption (train set)\")\n",
    "plt.xlabel(\"Correlation coefficient\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "selected_corr_features = corr_with_target[abs(corr_with_target) > 0.05].index.tolist()\n",
    "print(\"Kept after correlation filter:\", selected_corr_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea372a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SAMPLE for faster feature selection ---\n",
    "# Use only 10–15% of data for RFE (same feature relationships, far less compute)\n",
    "X_sample = X_train.sample(frac=0.15, random_state=42)\n",
    "y_sample = y_train.loc[X_sample.index]\n",
    "\n",
    "# --- LIGHTER RandomForest for selection ---\n",
    "rf_estimator = RandomForestClassifier(\n",
    "    n_estimators=300,  # fewer trees = faster\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# --- RFE wrapper ---\n",
    "rfe = RFE(estimator=rf_estimator, n_features_to_select=10, step=1)\n",
    "rfe.fit(X_sample[selected_corr_features], y_sample)\n",
    "\n",
    "selected_features = X_sample[selected_corr_features].columns[rfe.support_].tolist()\n",
    "print(\"\\nFinal selected features by RFE:\\n\", selected_features)\n",
    "\n",
    "# --- Importance scores for selected features ---\n",
    "importance_scores = pd.Series(\n",
    "    rfe.estimator_.feature_importances_,\n",
    "    index=X_sample[selected_corr_features].columns[rfe.support_]\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(x=importance_scores.values, y=importance_scores.index, palette=\"magma\")\n",
    "plt.title(\"Feature Importance (Random Forest - RFE, sample subset)\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save selection for reproducibility\n",
    "pd.Series(selected_features, name=\"selected_features\").to_csv(\"selected_features.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
