{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "f6906958",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "15ed507e",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "file = 'filtered_data.csv'\n",
        "df = pd.read_csv(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc47508f",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"First 5 rows of the dataset:\")\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5aebc1a",
      "metadata": {},
      "source": [
        "Summary Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3082ad1",
      "metadata": {},
      "outputs": [],
      "source": [
        "display(df.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d41d1b1d",
      "metadata": {},
      "source": [
        "Datatypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8058d553",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af729b40",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(df.isnull().sum())\n",
        "# No null values but if there were a few we would drop them\n",
        "df = df.dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b05a79c",
      "metadata": {},
      "source": [
        "# Outliers #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e6bc0cb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate and display outliers using IQR method for numerical columns\n",
        "numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "for col in numerical_cols:\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    \n",
        "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
        "    \n",
        "    print(f\"{col}:\")\n",
        "    print(f\"  Outlier count: {len(outliers)}\")\n",
        "    print(f\"  Outlier percentage: {len(outliers)/len(df)*100:.2f}%\")\n",
        "    print(f\"  Range: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
        "    if len(outliers) > 0:\n",
        "        print(f\"  Min outlier: {outliers[col].min():.2f}\")\n",
        "        print(f\"  Max outlier: {outliers[col].max():.2f}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cabab73b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create visualizations for outliers\n",
        "fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, col in enumerate(numerical_cols[:12]):  # Show first 12 numerical columns\n",
        "    # Calculate outlier bounds for this column\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    \n",
        "    # Create box plot\n",
        "    sns.boxplot(data=df, y=col, ax=axes[i])\n",
        "    axes[i].set_title(f'{col} - Outliers Detection')\n",
        "    axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Create additional plots for remaining numerical columns if any\n",
        "if len(numerical_cols) > 12:\n",
        "    remaining_cols = numerical_cols[12:]\n",
        "    n_remaining = len(remaining_cols)\n",
        "    n_cols = 4\n",
        "    n_rows = (n_remaining + n_cols - 1) // n_cols\n",
        "    \n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5*n_rows))\n",
        "    if n_rows == 1:\n",
        "        axes = [axes] if n_remaining == 1 else axes\n",
        "    else:\n",
        "        axes = axes.flatten()\n",
        "    \n",
        "    for i, col in enumerate(remaining_cols):\n",
        "        sns.boxplot(data=df, y=col, ax=axes[i])\n",
        "        axes[i].set_title(f'{col} - Outliers Detection')\n",
        "        axes[i].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Hide empty subplots\n",
        "    for j in range(i+1, len(axes)):\n",
        "        axes[j].set_visible(False)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "148a64f5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Variable groups from dataset documentation ---\n",
        "CONTINUOUS_VARS = [\n",
        "    \"age\", \"height\", \"weight\", \"sight_left\", \"sight_right\",\n",
        "    \"SBP\", \"DBP\", \"BLDS\", \"tot_chole\", \"HDL_chole\", \"LDL_chole\",\n",
        "    \"triglyceride\", \"hemoglobin\", \"serum_creatinine\",\n",
        "    \"SGOT_AST\", \"SGOT_ALT\", \"gamma_GTP\", \"waistline\"\n",
        "]\n",
        "\n",
        "ORDINAL_VARS = [\"urine_protein\"]  # exclude from IQR\n",
        "CATEGORICAL_VARS = [\"sex\", \"hear_left\", \"hear_right\", \"SMK_stat_type_cd\", \"DRK_YN\"]\n",
        "\n",
        "# Use intersection with actual columns\n",
        "cont_cols = [c for c in CONTINUOUS_VARS if c in df.columns]\n",
        "\n",
        "def iqr_bounds(s: pd.Series):\n",
        "    s = pd.to_numeric(s, errors=\"coerce\").dropna()\n",
        "    q1, q3 = s.quantile([0.25, 0.75])\n",
        "    iqr = q3 - q1\n",
        "    low = q1 - 1.5 * iqr\n",
        "    high = q3 + 1.5 * iqr\n",
        "    return low, high"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "249c30ca",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove (clear errors)\n",
        "\n",
        "df_clean = df.copy()\n",
        "\n",
        "REMOVE_RULES = {\n",
        "    \"waistline\": lambda s: s > 200,  # e.g., 1000 cm entries\n",
        "    \"serum_creatinine\": lambda s: s > 20,  # mg/dL, implausible highs\n",
        "    \"SGOT_AST\": lambda s: s > 9000,  # extreme enzyme typos\n",
        "    \"SGOT_ALT\": lambda s: s > 9000,\n",
        "    \"sight_left\": lambda s: s >= 5,  # 10.0 eyesight entries -> remove\n",
        "    \"sight_right\": lambda s: s >= 5,\n",
        "}\n",
        "\n",
        "# Apply removal rules\n",
        "mask_keep = pd.Series(True, index=df_clean.index)\n",
        "remove_report = []\n",
        "for col, rule in REMOVE_RULES.items():\n",
        "    if col in df_clean.columns:\n",
        "        bad = rule(pd.to_numeric(df_clean[col], errors=\"coerce\"))\n",
        "        n_bad = int(bad.sum())\n",
        "        if n_bad > 0:\n",
        "            mask_keep &= ~bad\n",
        "            remove_report.append((col, n_bad))\n",
        "\n",
        "df_clean = df_clean.loc[mask_keep]\n",
        "print(\"Rows removed (by column):\", remove_report)\n",
        "\n",
        "# =========================\n",
        "# 2) CAP (winsorize at IQR)\n",
        "# =========================\n",
        "cap_report = []\n",
        "for col in cont_cols:\n",
        "    if col in df_clean.columns:\n",
        "        low, high = iqr_bounds(df_clean[col])\n",
        "        before = pd.to_numeric(df_clean[col], errors=\"coerce\")\n",
        "        n_low = int((before < low).sum())\n",
        "        n_high = int((before > high).sum())\n",
        "        df_clean[col] = before.clip(lower=low, upper=high)\n",
        "        cap_report.append({\n",
        "            \"column\": col,\n",
        "            \"capped_low\": n_low,\n",
        "            \"capped_high\": n_high\n",
        "        })\n",
        "\n",
        "print(pd.DataFrame(cap_report))\n",
        "\n",
        "# ==============================\n",
        "# 3) TRANSFORM (reduce skew with log1p)\n",
        "# ==============================\n",
        "LOG1P_VARS = [\n",
        "    v for v in [\n",
        "        \"weight\", \"BLDS\", \"tot_chole\", \"HDL_chole\", \"LDL_chole\",\n",
        "        \"triglyceride\", \"SGOT_AST\", \"SGOT_ALT\", \"gamma_GTP\"\n",
        "    ]\n",
        "    if v in df_clean.columns\n",
        "]\n",
        "\n",
        "transform_report = []\n",
        "for col in LOG1P_VARS:\n",
        "    x = pd.to_numeric(df_clean[col], errors=\"coerce\")\n",
        "    n_neg = int((x < 0).sum())\n",
        "    if n_neg:\n",
        "        x = x.clip(lower=0)  # ensure non-negative before log1p\n",
        "    df_clean[col] = np.log1p(x)\n",
        "    transform_report.append({\n",
        "        \"column\": col,\n",
        "        \"negatives_clipped\": n_neg\n",
        "    })\n",
        "\n",
        "print(pd.DataFrame(transform_report))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73fa0759",
      "metadata": {},
      "source": [
        "# Feature extracting"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfc25e9e",
      "metadata": {},
      "source": [
        "Feature extraction after stabilizing data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "ae9775c6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Derived features (feature extraction)\n",
        "df_clean['BMI'] = df_clean['weight'] / (df_clean['height'] / 100) ** 2\n",
        "df_clean['pulse_pressure'] = df_clean['SBP'] - df_clean['DBP']\n",
        "df_clean['LDL_to_HDL'] = df_clean['LDL_chole'] / df_clean['HDL_chole']\n",
        "df_clean['TG_to_HDL'] = df_clean['triglyceride'] / df_clean['HDL_chole']\n",
        "df_clean['sight_avg'] = (df_clean['sight_left'] + df_clean['sight_right']) / 2\n",
        "df_clean['hearing_avg'] = (df_clean['hear_left'] + df_clean['hear_right']) / 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e1c7eb3",
      "metadata": {},
      "source": [
        "Data Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7216e7c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make a copy to avoid changing the original dataset\n",
        "df_encoded = df_clean.copy()\n",
        "df_encoded.drop(columns=['hear_left','hear_right','sight_left','sight_right'], inplace=True)\n",
        "\n",
        "# Ordinal encoding: urine_protein (1 to 6 is ordered)\n",
        "df_encoded[\"urine_protein\"] = df_encoded[\"urine_protein\"].astype(int)\n",
        "\n",
        "# One-hot encoding for nominal variables\n",
        "nominal_vars = [\"sex\", \"SMK_stat_type_cd\", \"DRK_YN\"]\n",
        "\n",
        "df_encoded = pd.get_dummies(df_encoded, columns=nominal_vars, drop_first=True)\n",
        "\n",
        "# Show only the newly created encoded columns\n",
        "encoded_cols = [col for col in df_encoded.columns if any(var in col for var in nominal_vars)] + [\"urine_protein\"]\n",
        "print(df_encoded[encoded_cols].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80fb1d5b",
      "metadata": {},
      "source": [
        "Feature scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1caec157",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Columns to show\n",
        "cols_to_show = [\"age\", \"weight\", \"waistline\", \"SBP\", \"triglyceride\"]\n",
        "\n",
        "# Before scaling\n",
        "summary_before = df_encoded[cols_to_show].describe().round(2)\n",
        "\n",
        "# After scaling\n",
        "scaler = StandardScaler()\n",
        "df_scaled = df_encoded.copy()\n",
        "df_scaled[cols_to_show] = scaler.fit_transform(df_encoded[cols_to_show])\n",
        "summary_after = df_scaled[cols_to_show].describe().round(2)\n",
        "\n",
        "# Display summaries neatly in notebook\n",
        "display(Markdown(\"### Summary statistics before scaling\"))\n",
        "display(summary_before)\n",
        "\n",
        "display(Markdown(\"### Summary statistics after standardization (Z-score)\"))\n",
        "display(summary_after)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f36808b0",
      "metadata": {},
      "source": [
        "Data splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dd2e63a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set 'DRK_YN' (Drinker or Not) as target variable — use the encoded dataframe\n",
        "# If one-hot encoding created \"DRK_YN_Y\", use that as your target\n",
        "target_col = \"DRK_YN_Y\" if \"DRK_YN_Y\" in df_encoded.columns else \"DRK_YN\"\n",
        "\n",
        "# Separate features (X) and label (y)\n",
        "X = df_encoded.drop(columns=[target_col])\n",
        "y = df_encoded[target_col]\n",
        "\n",
        "# Split the data into training and testing sets (80–20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(\"Training set shape (X_train):\", X_train.shape)\n",
        "print(\"Testing set shape (X_test):\", X_test.shape)\n",
        "print(\"Training set shape (y_train):\", y_train.shape)\n",
        "print(\"Testing set shape (y_test):\", y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9575b229",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_encoded"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cd5df75",
      "metadata": {},
      "source": [
        "### Correlation filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de5c1839",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Step 1 Correlation-based filtering ---\n",
        "corr_df = X_train.copy()\n",
        "corr_df['target'] = y_train\n",
        "\n",
        "corr_matrix = corr_df.corr(numeric_only=True)\n",
        "corr_with_target = corr_matrix['target'].drop('target').sort_values(ascending=False)\n",
        "\n",
        "plt.figure(figsize=(8,10))\n",
        "sns.barplot(x=corr_with_target.values, y=corr_with_target.index, palette=\"viridis\")\n",
        "plt.title(\"Feature correlations with alcohol consumption (train set)\")\n",
        "plt.xlabel(\"Correlation coefficient\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "selected_corr_features = corr_with_target[abs(corr_with_target) > 0.05].index.tolist()\n",
        "print(\"Kept after correlation filter:\", selected_corr_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dea372a8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- SAMPLE for faster feature selection ---\n",
        "# Use only 10–15% of data for RFE (same feature relationships, far less compute)\n",
        "X_sample = X_train.sample(frac=0.15, random_state=42)\n",
        "y_sample = y_train.loc[X_sample.index]\n",
        "\n",
        "# --- LIGHTER RandomForest for selection ---\n",
        "rf_estimator = RandomForestClassifier(\n",
        "    n_estimators=300,  # fewer trees = faster\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# --- RFE wrapper ---\n",
        "rfe = RFE(estimator=rf_estimator, n_features_to_select=10, step=1)\n",
        "rfe.fit(X_sample[selected_corr_features], y_sample)\n",
        "\n",
        "selected_features = X_sample[selected_corr_features].columns[rfe.support_].tolist()\n",
        "print(\"\\nFinal selected features by RFE:\\n\", selected_features)\n",
        "\n",
        "# --- Importance scores for selected features ---\n",
        "importance_scores = pd.Series(\n",
        "    rfe.estimator_.feature_importances_,\n",
        "    index=X_sample[selected_corr_features].columns[rfe.support_]\n",
        ").sort_values(ascending=False)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.barplot(x=importance_scores.values, y=importance_scores.index, palette=\"magma\")\n",
        "plt.title(\"Feature Importance (Random Forest - RFE, sample subset)\")\n",
        "plt.xlabel(\"Importance Score\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save selection for reproducibility\n",
        "pd.Series(selected_features, name=\"selected_features\").to_csv(\"selected_features.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c285759e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "import pandas as pd\n",
        "\n",
        "# --- Logistic Regression ---\n",
        "log_reg = LogisticRegression(max_iter=500)\n",
        "log_reg.fit(X_train[selected_features], y_train)\n",
        "\n",
        "y_pred = log_reg.predict(X_test[selected_features])\n",
        "y_prob = log_reg.predict_proba(X_test[selected_features])[:, 1]\n",
        "\n",
        "log_reg_results = {\n",
        "    \"Model\": \"Logistic Regression\",\n",
        "    \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "    \"F1\": f1_score(y_test, y_pred),\n",
        "    \"ROC-AUC\": roc_auc_score(y_test, y_prob)\n",
        "}\n",
        "\n",
        "print(pd.DataFrame([log_reg_results]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a75d23b1",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "import pandas as pd\n",
        "\n",
        "# --- Decision Tree ---\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "tree.fit(X_train[selected_features], y_train)\n",
        "\n",
        "y_pred = tree.predict(X_test[selected_features])\n",
        "y_prob = tree.predict_proba(X_test[selected_features])[:, 1]\n",
        "\n",
        "tree_results = {\n",
        "    \"Model\": \"Decision Tree\",\n",
        "    \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "    \"F1\": f1_score(y_test, y_pred),\n",
        "    \"ROC-AUC\": roc_auc_score(y_test, y_prob)\n",
        "}\n",
        "\n",
        "print(pd.DataFrame([tree_results]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9d38114",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "import pandas as pd\n",
        "\n",
        "# --- Random Forest ---\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "rf.fit(X_train[selected_features], y_train)\n",
        "\n",
        "y_pred = rf.predict(X_test[selected_features])\n",
        "y_prob = rf.predict_proba(X_test[selected_features])[:, 1]\n",
        "\n",
        "rf_results = {\n",
        "    \"Model\": \"Random Forest\",\n",
        "    \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "    \"F1\": f1_score(y_test, y_pred),\n",
        "    \"ROC-AUC\": roc_auc_score(y_test, y_prob)\n",
        "}\n",
        "\n",
        "print(pd.DataFrame([rf_results]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d01a5dba",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "# Downsample for faster testing\n",
        "X_small = X_train[selected_features].sample(frac=0.1, random_state=42)\n",
        "y_small = y_train.loc[X_small.index]\n",
        "\n",
        "# --- SVM ---\n",
        "svm_model = SVC(kernel='rbf', probability=True, gamma='scale', random_state=42)\n",
        "\n",
        "svm_model.fit(X_small, y_small)\n",
        "\n",
        "y_pred = svm_model.predict(X_test[selected_features])\n",
        "y_prob = svm_model.predict_proba(X_test[selected_features])[:, 1]\n",
        "\n",
        "svm_results = {\n",
        "    \"Model\": \"SVM (RBF Kernel)\",\n",
        "    \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "    \"F1\": f1_score(y_test, y_pred),\n",
        "    \"ROC-AUC\": roc_auc_score(y_test, y_prob)\n",
        "}\n",
        "print(svm_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24ab9e01",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "# --- Neural Network ---\n",
        "nn_model = MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=200, early_stopping=True, random_state=42)\n",
        "\n",
        "nn_model.fit(X_train[selected_features], y_train)\n",
        "\n",
        "y_pred = nn_model.predict(X_test[selected_features])\n",
        "y_prob = nn_model.predict_proba(X_test[selected_features])[:, 1]\n",
        "\n",
        "nn_results = {\n",
        "    \"Model\": \"Neural Network\",\n",
        "    \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "    \"F1\": f1_score(y_test, y_pred),\n",
        "    \"ROC-AUC\": roc_auc_score(y_test, y_prob)\n",
        "}\n",
        "print(nn_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ece77ed7",
      "metadata": {},
      "source": [
        "## TASK 7 - BAGGING N BOOSTING"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a2126ac",
      "metadata": {},
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "b2351da4",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "4ac93177",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_encoded"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ae9254a",
      "metadata": {},
      "source": [
        "### Split and fix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "58688850",
      "metadata": {},
      "outputs": [],
      "source": [
        "X = df_encoded.drop(columns=['DRK_YN_Y'])\n",
        "y = df_encoded['DRK_YN_Y']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "181cb23e",
      "metadata": {},
      "source": [
        "### Bagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "77da121d",
      "metadata": {},
      "outputs": [],
      "source": [
        "bag_model = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(max_depth=5, random_state=42),\n",
        "    n_estimators=50,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "bag_model.fit(X_train, y_train)\n",
        "y_pred_bag = bag_model.predict(X_test)\n",
        "acc_bag = accuracy_score(y_test, y_pred_bag)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "714166f6",
      "metadata": {},
      "source": [
        "### Boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "50b8ef00",
      "metadata": {},
      "outputs": [],
      "source": [
        "# adaboost boosting\n",
        "ada_model = AdaBoostClassifier(\n",
        "    estimator=DecisionTreeClassifier(max_depth=1, random_state=42),\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "ada_model.fit(X_train, y_train)\n",
        "y_pred_ada = ada_model.predict(X_test)\n",
        "acc_ada = accuracy_score(y_test, y_pred_ada)\n",
        "\n",
        "# gradient boosting\n",
        "gb_model = GradientBoostingClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3,\n",
        "    random_state=42\n",
        ")\n",
        "gb_model.fit(X_train, y_train)\n",
        "y_pred_gb = gb_model.predict(X_test)\n",
        "acc_gb = accuracy_score(y_test, y_pred_gb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91d986bd",
      "metadata": {},
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42765e21",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Bagging accuracy:\", acc_bag)\n",
        "print(\"AdaBoost accuracy:\", acc_ada)\n",
        "print(\"Gradient Boosting accuracy:\", acc_gb)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
